services: 
  triton-inference-server: 
    image: nvcr.io/nvidia/tritonserver:24.01-py3 
    container_name: triton-inference
    restart: always
    command: tritonserver --model-repository=/models
    volumes:
      - /home/simin/triton_inference/docs/examples/model_repository:/models
    expose: 
      - 8000 
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    deploy: 
      resources: 
        reservations: 
          devices: 
            - driver: nvidia 
              count: 1 
              capabilities: [gpu]
    networks:
      - app-network